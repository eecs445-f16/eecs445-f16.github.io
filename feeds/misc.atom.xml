<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EECS 445: Introduction to Machine Learning</title><link href="http://eecs445-f16.github.io/" rel="alternate"></link><link href="http://eecs445-f16.github.io/feeds/misc.atom.xml" rel="self"></link><id>http://eecs445-f16.github.io/</id><updated>2016-10-12T00:00:00-04:00</updated><entry><title>Lecture 11, Decision Trees &amp; Information Theory</title><link href="http://eecs445-f16.github.io/lecture-11-decision-trees-information-theory.html" rel="alternate"></link><published>2016-10-12T00:00:00-04:00</published><updated>2016-10-12T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-10-12:lecture-11-decision-trees-information-theory.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy and mutual information&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §2.8:&lt;/strong&gt; Information Theory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §16.1-16.2:&lt;/strong&gt; Classification and Regression Trees&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §1.6:&lt;/strong&gt; Information Theory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blog Post:&lt;/strong&gt; Aldo Cortesi, &lt;a href="https://corte.si/posts/visualisation/entropy/index.html"&gt;"Visualizing Entropy in Binary Files"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 10. Bias-Variance Tradeoff</title><link href="http://eecs445-f16.github.io/lecture-10-bias-variance-tradeoff.html" rel="alternate"></link><published>2016-10-10T00:00:00-04:00</published><updated>2016-10-10T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-10-10:lecture-10-bias-variance-tradeoff.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bias vs Variance&lt;/li&gt;
&lt;li&gt;Visualization of Bias vs Variance&lt;/li&gt;
&lt;li&gt;Model complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §3.2:&lt;/strong&gt; The Bias-Variance Decomposition&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 9. Support Vector Machines, Part II</title><link href="http://eecs445-f16.github.io/lecture-9-support-vector-machines-part-ii.html" rel="alternate"></link><published>2016-10-05T00:00:00-04:00</published><updated>2016-10-05T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-10-05:lecture-9-support-vector-machines-part-ii.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimal soft-margin hyperplane objective&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 8. Support Vector Machines, Part I</title><link href="http://eecs445-f16.github.io/lecture-8-support-vector-machines-part-i.html" rel="alternate"></link><published>2016-10-03T00:00:00-04:00</published><updated>2016-10-03T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-10-03:lecture-8-support-vector-machines-part-i.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find a linear classifier&lt;/li&gt;
&lt;li&gt;Maximize the margin&lt;/li&gt;
&lt;li&gt;Get an optimization problem with constraints&lt;/li&gt;
&lt;li&gt;Variables have some semantics&lt;/li&gt;
&lt;li&gt;No corresponding probabilistic model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §14.5:&lt;/strong&gt; Support Vector Machines&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229, Lecture Notes #3:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf"&gt;Part V, Support Vector Machines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 7. Naive Bayes</title><link href="http://eecs445-f16.github.io/lecture-7-naive-bayes.html" rel="alternate"></link><published>2016-09-28T00:00:00-04:00</published><updated>2016-09-28T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-28:lecture-7-naive-bayes.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understand the difference between generative and discriminative classifiers&lt;ul&gt;
&lt;li&gt;Know which models we've used belong to which category&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Naive Bayes Classifiers&lt;ul&gt;
&lt;li&gt;Understand the conditional independence assumption and implications&lt;/li&gt;
&lt;li&gt;Know how to write down the likelihood function&lt;/li&gt;
&lt;li&gt;Be able to compute MLE/MAP estimates by hand&lt;/li&gt;
&lt;li&gt;Understand Laplace smoothing and the problem it solves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compare Logistic Regression and Naive Bayes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §3.1-3.4:&lt;/strong&gt; Generative Models for Discrete Data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §3.5:&lt;/strong&gt; Naive Bayes Classifiers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §8.6:&lt;/strong&gt; Generative vs. Discriminative Classifiers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229, Lecture Notes #2:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/notes/cs229-notes2.pdf"&gt;Part IV, Generative Learning Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Advanced Reading, only if you're interested:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; Ng &amp;amp; Jordan 2001, &lt;a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"&gt;On Discriminative vs. Generative Classifiers:  A Comparison of Logistic Regression and Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; Zhang, H., 2004. &lt;a href="http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf"&gt;"The optimality of naive Bayes"&lt;/a&gt;. AA, 1(2), p.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; Domingos, P. and Pazzani, M., 1997. &lt;a href="http://link.springer.com/article/10.1023/A:1007413511361"&gt;"On the optimality of the simple Bayesian classifier under zero-one loss"&lt;/a&gt;. Machine learning, 29(2-3), pp.103-130.&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 6. Logistic Regression</title><link href="http://eecs445-f16.github.io/lecture-6-logistic-regression.html" rel="alternate"></link><published>2016-09-26T00:00:00-04:00</published><updated>2016-09-26T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-26:lecture-6-logistic-regression.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Undersand the supervised learning classification problem formulation&lt;/li&gt;
&lt;li&gt;Understand the probabilistic interpretation of logistic regression&lt;ul&gt;
&lt;li&gt;Know why we use the sigmoid function rather than a hard threshold&lt;/li&gt;
&lt;li&gt;Write down the likelihood function&lt;/li&gt;
&lt;li&gt;Be able to take the gradient of the negative log-likelihood objective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use Newton's method to find the maximum likelihood parameter estimate&lt;ul&gt;
&lt;li&gt;Understand why Newton's method applies here&lt;/li&gt;
&lt;li&gt;Understand the difference between gradient descent and Newton's method&lt;/li&gt;
&lt;li&gt;Understand Newton's method geometrically for one-dimensional problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Know that there is no closed-form solution for logistic regression&lt;/li&gt;
&lt;li&gt;Understand logistic regression as a linear classifier&lt;/li&gt;
&lt;li&gt;Know how logistic regression can be generalized to softmax regression for multiclass problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §8.1-8.3&lt;/strong&gt; Logistic Regression&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §4.2&lt;/strong&gt;: Probabilistic Generative Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §4.3&lt;/strong&gt;: Probabilistic Discriminative Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229, Lecture Notes #1:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf"&gt;Part II, Classification &amp;amp; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229, Supplemental Notes #1:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/extra-notes/loss-functions.pdf"&gt;Binary Classification &amp;amp; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 5. Linear Regression, Part II</title><link href="http://eecs445-f16.github.io/lecture-5-linear-regression-part-ii.html" rel="alternate"></link><published>2016-09-21T00:00:00-04:00</published><updated>2016-09-21T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-21:lecture-5-linear-regression-part-ii.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overfitting and the need for regularization&lt;ul&gt;
&lt;li&gt;Write the objective function for lasso and ridge regression&lt;/li&gt;
&lt;li&gt;Use matrix calculus to find the gradient of the regularized objective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Understand the probabilistic interpretation of linear regression&lt;ul&gt;
&lt;li&gt;MLE formulation of linear regression objective&lt;/li&gt;
&lt;li&gt;MAP formulation of regularized linear regression&lt;ul&gt;
&lt;li&gt;Different priors correspond to different regularization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Understand probabilistic modeling at a high level&lt;ul&gt;
&lt;li&gt;Goals and assumptions of maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Goals and assumptions of MAP estimation&lt;/li&gt;
&lt;li&gt;Priors, likelihoods, and posteriors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §7.5:&lt;/strong&gt; Ridge Regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Advanced Reading, only if you're interested:
&lt;em&gt; &lt;strong&gt;Murphy, §13.3:&lt;/strong&gt; $\ell_1$ Regularization Basics
&lt;/em&gt; &lt;strong&gt;Murphy, §13.4:&lt;/strong&gt; $\ell_1$ Regularization Algorithms&lt;/p&gt;</summary></entry><entry><title>Lecture 4. Linear Regression, Part I</title><link href="http://eecs445-f16.github.io/lecture-4-linear-regression-part-i.html" rel="alternate"></link><published>2016-09-19T00:00:00-04:00</published><updated>2016-09-19T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-19:lecture-4-linear-regression-part-i.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning regression problem&lt;/li&gt;
&lt;li&gt;Understand the assumptions made by the model of linear regression&lt;/li&gt;
&lt;li&gt;Understand the difference between input $x$ and features $\phi(x)$&lt;/li&gt;
&lt;li&gt;Intuitively understand the least squares objective function&lt;/li&gt;
&lt;li&gt;Use gradient descent to minimize the objective function&lt;ul&gt;
&lt;li&gt;Use matrix algebra to compute the gradient of the objective&lt;/li&gt;
&lt;li&gt;Pros and cons of batch vs. stochastic gradient descent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use the normal equations to find the least squares solution&lt;ul&gt;
&lt;li&gt;Know how to derive the pseudoinverse&lt;/li&gt;
&lt;li&gt;Understand the normal equations geometrically&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Murphy, §7.1-7.3:&lt;/strong&gt; Linear Regression&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §1.1:&lt;/strong&gt; Polynomial Curve Fitting Example&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bishop, §3.1:&lt;/strong&gt; Linear Basis Function Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229, Lecture Notes #1:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf"&gt;Part I, Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Advanced Reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Blog Post:&lt;/strong&gt; Moritz Hardt, &lt;a href="http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html"&gt;"The Zen of Gradient Descent"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 03, Probability Review &amp; Intro to Optimization</title><link href="http://eecs445-f16.github.io/lecture-03-probability-review-intro-to-optimization.html" rel="alternate"></link><published>2016-09-14T00:00:00-04:00</published><updated>2016-09-14T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-14:lecture-03-probability-review-intro-to-optimization.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Essential concepts in probability&lt;ul&gt;
&lt;li&gt;You should be able to work with both discrete distributions AND continuous distributions.  Understand pdfs and cdfs.&lt;/li&gt;
&lt;li&gt;What is the difference between a random variable and a distribution?&lt;/li&gt;
&lt;li&gt;Independent random variables and events.&lt;/li&gt;
&lt;li&gt;Common distributions and their properties (mean, variance, etc.)&lt;ul&gt;
&lt;li&gt;Bernoulli&lt;/li&gt;
&lt;li&gt;Binomial&lt;/li&gt;
&lt;li&gt;Multinomial&lt;/li&gt;
&lt;li&gt;Poisson&lt;/li&gt;
&lt;li&gt;Categorical&lt;/li&gt;
&lt;li&gt;Gaussian / Normal / Multivariate Normal&lt;/li&gt;
&lt;li&gt;Beta&lt;/li&gt;
&lt;li&gt;Dirichlet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Random vectors.  (Vectors whose elements are random variables)&lt;/li&gt;
&lt;li&gt;Expectation, variance, and covariance of random variables.&lt;/li&gt;
&lt;li&gt;Conditional expectation and variance.&lt;/li&gt;
&lt;li&gt;Functions of random variables (compute expectation of functions of random variables, etc.)&lt;/li&gt;
&lt;li&gt;Joint distributions.  Marginalization.  Marginal distributions.&lt;/li&gt;
&lt;li&gt;Bayes' theorem.&lt;/li&gt;
&lt;li&gt;Law of Total Probability&lt;/li&gt;
&lt;li&gt;You should be comfortable with conditional probability.&lt;/li&gt;
&lt;li&gt;Infinite sequences of random variables.&lt;/li&gt;
&lt;li&gt;Basic understanding of likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Convex functions: definition and properties&lt;/li&gt;
&lt;li&gt;Constrained vs. unconstrained optimization&lt;/li&gt;
&lt;li&gt;Basic understanding of Lagrange duality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CS229:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/section/cs229-prob.pdf"&gt;Review of Probability Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/section/cs229-cvxopt.pdf"&gt;Convex Optimization Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/section/cs229-cvxopt2.pdf"&gt;Convex Optimization Overview Part II&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 02, Linear Algebra Review</title><link href="http://eecs445-f16.github.io/lecture-02-linear-algebra-review.html" rel="alternate"></link><published>2016-09-12T00:00:00-04:00</published><updated>2016-09-12T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-12:lecture-02-linear-algebra-review.html</id><summary type="html">&lt;p&gt;Learning Objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic matrix operations.  You should be able to interpret all of these geometrically AND write down generic formulas for each.&lt;ul&gt;
&lt;li&gt;Vector addition, subtraction, and scaling.&lt;/li&gt;
&lt;li&gt;Matrix-vector multiplication, and interpretations in terms of the rows and columns of the matrix.&lt;/li&gt;
&lt;li&gt;Matrix-matrix multiplication, and interpretations.&lt;/li&gt;
&lt;li&gt;Dot products, including how they relate to projections and the angle between vectors.  More generally, inner products.&lt;/li&gt;
&lt;li&gt;Projection onto a vector or onto a subspace.&lt;/li&gt;
&lt;li&gt;Matrix transposes.  Transpose of products, transpose of sums.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Basic Matrix Properties&lt;ul&gt;
&lt;li&gt;Understand the rank of a matrix both algebraically and geometrically.  What does it mean for a matrix to have full rank?&lt;/li&gt;
&lt;li&gt;Be able to describe the image and kernel of a matrix using set notation.&lt;/li&gt;
&lt;li&gt;Know all of the common equivalent conditions for a matrix to be invertible, in terms of the eigenvalues, the columns, the determinant, etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vector spaces&lt;ul&gt;
&lt;li&gt;Definition of an abstract vector space / subspace.&lt;/li&gt;
&lt;li&gt;Definition of linearly independent vectors&lt;/li&gt;
&lt;li&gt;Definition of basis, span, and dimension&lt;/li&gt;
&lt;li&gt;Orthogonality and orthogonal / orthonormal bases&lt;/li&gt;
&lt;li&gt;Know how to compute the coordinate vector for an element of an abstract vector space, with respect to a known basis.&lt;/li&gt;
&lt;li&gt;Gram-Schmidt orthogonalization&lt;/li&gt;
&lt;li&gt;Know what a linear transformation is.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Norms&lt;ul&gt;
&lt;li&gt;Definition of a norm, and the properties that every norm must satisfy.&lt;/li&gt;
&lt;li&gt;Understand and interpret geometrically the 1-norm, 2-norm, and infinity-norms.  More generally, the p-norms.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Special matrices.  Know the properties and common use cases for each.  For example, what can we say about the eigenvalues/eigenvectors for each special matrix below?  How do we multiply these matrices or take powers?  What can we say about the rank?&lt;ul&gt;
&lt;li&gt;Symmetric / Hermitian matrices.  &lt;/li&gt;
&lt;li&gt;Orthogonal / unitary matrices&lt;/li&gt;
&lt;li&gt;Diagonal matrices&lt;/li&gt;
&lt;li&gt;Triangular matrix&lt;/li&gt;
&lt;li&gt;Positive-definite and positive-semidefinite matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eigenvalues / eigenvectors&lt;ul&gt;
&lt;li&gt;Geometric interpretation&lt;/li&gt;
&lt;li&gt;Matrix diagonalization.  Which matrices are diagonalizable and how can you tell?&lt;/li&gt;
&lt;li&gt;Spectral theorem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Singular value decomposition&lt;ul&gt;
&lt;li&gt;Geometric interpretation (the image of the unit circle under every matrix is a hyperellipse)&lt;/li&gt;
&lt;li&gt;What does the SVD look like for the special matrices listed above?&lt;/li&gt;
&lt;li&gt;How can we interpret the singular values?&lt;/li&gt;
&lt;li&gt;How does the SVD relate to the rank of a matrix?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helpful Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://umich.instructure.com/courses/86539/files/folder/unsw-math2501-linear-algebra-notes"&gt;UNSW MATH 2501 Linear Algebra Notes&lt;/a&gt; on Canvas.  These are very well-written with lots of examples.  You should be comfortable with the material from the first 7-8 chapters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Book:&lt;/strong&gt;  &lt;a href="https://www.amazon.com/Linear-Algebra-Right-Undergraduate-Mathematics/dp/0387982582"&gt;"Linear Algebra Done Right"&lt;/a&gt; by Sheldon Axler&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Notes:&lt;/strong&gt;  &lt;a href="http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf"&gt;"Matrix Differentiation"&lt;/a&gt; by RJ Barnes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CS229:&lt;/strong&gt; &lt;a href="http://cs229.stanford.edu/section/cs229-linalg.pdf"&gt;Linear Algebra Review and Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Lecture 1, Introduction to Machine Learning</title><link href="http://eecs445-f16.github.io/lecture-1-introduction-to-machine-learning.html" rel="alternate"></link><published>2016-09-07T00:00:00-04:00</published><updated>2016-09-07T00:00:00-04:00</updated><author><name>EECS 445 Course Staff</name></author><id>tag:eecs445-f16.github.io,2016-09-07:lecture-1-introduction-to-machine-learning.html</id><summary type="html"></summary></entry></feed>